Output from algorithm:

```
mteffeteller@mark-01:~/mse/final-project$ python3 q1.py 
SLPM revenue: 8314.433322095947 at k=50
SLPM revenue: 8330.532794100718 at k=75
SLPM revenue: 8399.008302852302 at k=100
SLPM revenue: 8758.690592783189 at k=150
SLPM revenue: 8830.13448332816 at k=200
SLPM revenue: 8515.760186017691 at k=250
SLPM revenue: 9181.860142399079 at k=350
SLPM revenue: 8915.79919908586 at k=500
SLPM revenue: 9038.665033972193 at k=1000
SLPM revenue: 4912.98496928448 at k=5000
Offline revenue: 9956.983278071393
```

Key findings:
- There is a bell curve to the sensivity ratio. With too little or too 
  much information being considered, we sacrifice valuable bids. We need
  to find the "sweet spot", which in the output of this algorithm is k=350
  which maximizes revenue. 

- Note, given the randomness of this experiment, we can expect slightly
  different results at each simulation

- We used the [HiGHS](https://ergo-code.github.io/HiGHS/stable/) for      
  calculating, so please reference this

Things written by Bard that I approve of:

Increasing k:

Pros:

More accurate estimates of prices: With more bids, the algorithm has a more diverse set of information to learn from, leading to more accurate estimates of the true prices. This can potentially increase revenue as the algorithm is able to allocate resources more efficiently.
Reduced regret: Regret refers to the difference between the revenue generated by the online algorithm and the optimal revenue that could have been achieved with perfect knowledge of all bids. Increasing k can decrease regret by providing a more comprehensive picture of the market.

Cons:

Higher computational cost: Solving the linear program with more bids requires more computational resources, which can be slow and resource-intensive for very large k values.
Potential for overfitting: With too many bids, the algorithm may overfit to the specific data points and not generalize well to unseen scenarios.
Decreasing k:

Pros:

Lower computational cost: Solving the linear program with fewer bids requires less computational resources, making it more efficient for real-time applications.
Reduced sensitivity to noise: With fewer bids, the algorithm is less affected by individual outliers or noisy data points.

Cons:

Less accurate estimates of prices: With fewer bids, the algorithm may have less accurate estimates of the true prices, leading to potential revenue loss.
Higher regret: A smaller k value can increase regret as the algorithm has a less complete picture of the market dynamics.
Overall, the optimal value of k depends on the specific application and the trade-off between accuracy, computational cost, and regret. In general, larger values of k will provide more accurate estimates of prices but will be more computationally expensive. Smaller values of k will be more efficient but may lead to less accurate estimates and higher regret.

Comparison to offline algorithm with 100% information:

An offline algorithm with 100% information about all bids will always achieve the optimal revenue. This is because it can make perfect allocation decisions based on the known prices, without any uncertainty or error. However, in real-world scenarios, such information is rarely available. The online SLPM algorithm aims to maximize revenue in situations where bids need to be evaluated and allocated on the fly, without complete knowledge of all future bids.

Therefore, comparing the revenue generated by SLPM with k to the offline optimal revenue provides a measure of the algorithm's performance and insight into the potential revenue loss due to limited information. The results show that even with a relatively small k value (e.g., 50), SLPM can achieve a significant portion of the offline optimal revenue. This suggests that it can be a viable and efficient approach for online revenue maximization in situations with limited information.


